{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn-one-hidden-layer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXz+KixoI2aF9Gcwq881uZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindhuvahinis/learning-djl/blob/main/nn_one_hidden_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQBce3kF5n5v"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v2 import *\n",
        "from public_tests import *\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = load_planar_dataset()"
      ],
      "metadata": {
        "id": "kqf285l653IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    n_x = X.shape[0]\n",
        "    n_h = 4\n",
        "    n_y = Y.shape[0]    \n",
        "    return (n_x, n_h, n_y)"
      ],
      "metadata": {
        "id": "fo9DoLDV54lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_X, t_Y = layer_sizes_test_case()\n",
        "(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\n",
        "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "print(\"The size of the output layer is: n_y = \" + str(n_y))\n",
        "\n",
        "layer_sizes_test(layer_sizes)"
      ],
      "metadata": {
        "id": "VPym-a2959Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"    \n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros(shape=(n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros(shape=(n_y, 1))\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "5kp2w93w6FRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2) \n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "metadata": {
        "id": "27_PCryA6Nri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A2, Y):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \n",
        "    \"\"\"\n",
        "    m = Y.shape[1] # number of examples\n",
        "\n",
        "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2), (1-Y))\n",
        "    cost = - np.sum(logprobs)\n",
        "    cost = float(np.squeeze(cost))  \n",
        "    \n",
        "    return cost"
      ],
      "metadata": {
        "id": "Y_fdDifT6Xks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "        \n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    dZ2= A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "metadata": {
        "id": "45M2Ibfr6hz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    W1 = copy.deepcopy(parameters['W1'])\n",
        "    b1 = copy.deepcopy(parameters['b1'])\n",
        "    W2 = copy.deepcopy(parameters['W2'])\n",
        "    b2 = copy.deepcopy(parameters['b2'])\n",
        "\n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "S1WfmOMP6sXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        cost = compute_cost(A2, Y)\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "        \n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "uxIleHvI6zbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.round(A2)    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "0Dku7eRR669r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
        "predictions = predict(parameters, X)"
      ],
      "metadata": {
        "id": "YbRk-uNf7AWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}